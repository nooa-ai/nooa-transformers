# Google Colab - Guia RÃ¡pido

## ğŸ¯ O que vamos fazer?

Treinar o GrammaticalBERT no Google Colab (GPU grÃ¡tis!) usando o dataset GLUE que jÃ¡ vem pronto.

**Boa notÃ­cia**: NÃ£o precisa preparar dataset! O Hugging Face baixa automaticamente. ğŸ‰

---

## ğŸ“‹ Passo a Passo

### 1ï¸âƒ£ Abrir o Google Colab

1. Acesse: https://colab.research.google.com
2. FaÃ§a login com sua conta Google
3. Clique em **File â†’ New Notebook**

### 2ï¸âƒ£ Ativar GPU GrÃ¡tis

1. No notebook, clique em **Runtime â†’ Change runtime type**
2. Em **Hardware accelerator**, selecione **GPU** (T4 grÃ¡tis)
3. Clique **Save**

### 3ï¸âƒ£ Copiar e Colar o CÃ³digo Abaixo

Copie todo o cÃ³digo do notebook `colab_training.ipynb` que eu vou criar agora.

---

## ğŸš€ OpÃ§Ãµes de Treinamento

### OpÃ§Ã£o A: Fine-tuning em GLUE (Recomendado para comeÃ§ar)

**O que Ã©**: Pegar um modelo BERT jÃ¡ treinado e ajustar para tarefas especÃ­ficas (sentiment analysis, etc)

**Dataset**: GLUE (jÃ¡ vem pronto via Hugging Face)
- SST-2: 67K exemplos (sentiment)
- MNLI: 393K exemplos (entailment)
- CoLA: 8.5K exemplos (gramaticalidade)
- ... mais 5 tarefas

**Tempo**: 10-30 minutos por tarefa (GPU grÃ¡tis T4)

**Custo**: $0 (grÃ¡tis)

**Preparar dataset?**: âŒ NÃƒO! Baixa automaticamente

```python
# Tudo automÃ¡tico:
from datasets import load_dataset
dataset = load_dataset("glue", "sst2")  # Pronto!
```

### OpÃ§Ã£o B: Pre-training do Zero (AvanÃ§ado)

**O que Ã©**: Treinar um modelo completamente do zero em texto genÃ©rico

**Dataset**: Precisa preparar corpus grande (Wikipedia, livros)
- Tamanho: 16GB+ de texto
- Formato: arquivos .txt

**Tempo**: Dias a semanas (mesmo com GPU)

**Custo**: $100-500 (precisa GPU paga tipo A100)

**Preparar dataset?**: âœ… SIM! Precisa corpus grande

**RecomendaÃ§Ã£o**: Comece com fine-tuning (OpÃ§Ã£o A)! Pre-training Ã© muito caro e demorado.

---

## ğŸ“Š Datasets DisponÃ­veis (Zero Setup)

### GLUE Benchmark (Recomendado)

```python
from datasets import load_dataset

# Sentiment Analysis (mais fÃ¡cil)
sst2 = load_dataset("glue", "sst2")
# 67K exemplos
# Input: "this movie is great"
# Output: positive/negative

# Natural Language Inference
mnli = load_dataset("glue", "mnli")
# 393K exemplos
# Input: premise + hypothesis
# Output: entailment/contradiction/neutral

# Gramaticalidade (interessante para GrammaticalBERT!)
cola = load_dataset("glue", "cola")
# 8.5K exemplos
# Input: sentence
# Output: grammatical/ungrammatical
```

### Outros Datasets Prontos

```python
# Question Answering
squad = load_dataset("squad")

# Translation
wmt = load_dataset("wmt14", "de-en")

# Summarization
cnn_dailymail = load_dataset("cnn_dailymail", "3.0.0")
```

**Tudo baixa automaticamente!** Sem preparaÃ§Ã£o manual.

---

## âš¡ Colab Free vs Pro

### Free (T4 GPU)
- **Custo**: $0
- **GPU**: Tesla T4 (16GB VRAM)
- **Tempo**: 12 horas por sessÃ£o
- **Velocidade**:
  - SST-2: ~20 min
  - MNLI: ~2 horas
- **LimitaÃ§Ã£o**: Desconecta apÃ³s 12h ou inatividade

### Pro ($10/mÃªs)
- **GPU**: A100 (Ã s vezes), melhor prioridade
- **Tempo**: 24 horas por sessÃ£o
- **Velocidade**:
  - SST-2: ~5 min
  - MNLI: ~30 min
- **Vantagem**: Menos desconexÃµes

**RecomendaÃ§Ã£o**: Comece com Free! Ã‰ suficiente para aprender.

---

## ğŸ“ PrÃ³ximos Passos

1. âœ… Abrir Colab e ativar GPU
2. âœ… Usar notebook que vou criar (`colab_training.ipynb`)
3. âœ… Rodar fine-tuning no SST-2 (20 min)
4. âœ… Ver os resultados
5. âœ… Testar outras tarefas GLUE

Depois vocÃª pode:
- Comparar GrammaticalBERT vs vanilla BERT
- Medir reduÃ§Ã£o de hallucinations
- Publicar resultados

---

## ğŸ“ Resumo

**Precisa preparar dataset?**
- Fine-tuning: âŒ NÃƒO (usa GLUE, jÃ¡ vem pronto)
- Pre-training: âœ… SIM (precisa corpus grande)

**RecomendaÃ§Ã£o**:
1. Comece com fine-tuning no SST-2 (sentiment)
2. Use Colab Free (suficiente)
3. Leva ~20 minutos
4. Depois teste outras tarefas

**PrÃ³ximo arquivo**: Vou criar `colab_training.ipynb` com cÃ³digo pronto para copiar e colar! ğŸš€
