{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ GrammaticalBERT Training - Google Colab\n",
    "\n",
    "**IMPORTANTE**: Execute as c√©lulas EM ORDEM! N√£o pule nenhuma c√©lula.\n",
    "\n",
    "## Setup R√°pido:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
    "2. Execute as c√©lulas uma por uma (Shift+Enter)\n",
    "3. Aguarde ~20 minutos para treinar no SST-2\n",
    "\n",
    "## O que vamos fazer:\n",
    "- Fine-tuning do GrammaticalBERT no dataset SST-2 (sentiment analysis)\n",
    "- Comparar com vanilla BERT\n",
    "- Medir accuracy, F1, e redu√ß√£o de hallucinations\n",
    "\n",
    "**Dataset**: Baixa automaticamente (sem prepara√ß√£o manual!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ C√©lula 1: Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.8' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/opt/homebrew/bin/python3.13 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"üîç Verificando ambiente...\\n\")\n",
    "\n",
    "# GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"‚úÖ GPU dispon√≠vel: {gpu_name}\")\n",
    "    print(f\"   VRAM: {vram:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU n√£o dispon√≠vel!\")\n",
    "    print(\"   V√° em: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    print(\"   Depois execute esta c√©lula novamente.\")\n",
    "\n",
    "# Python\n",
    "print(f\"\\n‚úÖ Python: {sys.version.split()[0]}\")\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ C√©lula 2: Clonar Reposit√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Verificar se j√° clonou\n",
    "if os.path.exists('/content/nooa-transformers'):\n",
    "    print(\"‚úÖ Reposit√≥rio j√° existe!\")\n",
    "else:\n",
    "    print(\"üì• Clonando reposit√≥rio...\")\n",
    "    !git clone https://github.com/nooa-ai/nooa-transformers.git\n",
    "    print(\"‚úÖ Reposit√≥rio clonado!\")\n",
    "\n",
    "# Mudar para o diret√≥rio raiz do projeto\n",
    "os.chdir('/content/nooa-transformers')\n",
    "print(f\"\\nüìÇ Diret√≥rio atual: {os.getcwd()}\")\n",
    "\n",
    "# Verificar estrutura\n",
    "print(\"\\nüìÅ Estrutura do projeto:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nüìÅ Estrutura de grammatical_transformers:\")\n",
    "!ls -la grammatical_transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ C√©lula 3: Instalar Depend√™ncias\n",
    "\n",
    "**IMPORTANTE**: Esta c√©lula pode levar 2-3 minutos. Aguarde at√© aparecer \"‚úÖ Instala√ß√£o completa!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\n# Garantir que estamos no diret√≥rio raiz do projeto (n√£o dentro de grammatical_transformers/)\nos.chdir('/content/nooa-transformers')\nprint(f\"üìÇ Diret√≥rio de instala√ß√£o: {os.getcwd()}\\n\")\n\nprint(\"üì¶ Instalando grammatical_transformers...\\n\")\n\n# Instalar o pacote do diret√≥rio raiz\n!pip install -e . -q\n\nprint(\"\\nüì¶ Instalando depend√™ncias de benchmark...\\n\")\n# Scipy √© necess√°rio para pearson correlation (STS-B task)\n!pip install datasets accelerate evaluate scikit-learn scipy -q\n\n# Verificar instala√ß√£o\nprint(\"\\nüîç Verificando instala√ß√£o...\")\ntry:\n    import grammatical_transformers\n    print(f\"‚úÖ grammatical_transformers instalado: v{grammatical_transformers.__version__}\")\n    print(f\"   Localiza√ß√£o: {grammatical_transformers.__file__}\")\nexcept ImportError as e:\n    print(f\"‚ùå Erro ao importar: {e}\")\n    print(\"\\n‚ö†Ô∏è  Diagn√≥stico:\")\n    print(f\"   sys.path: {sys.path[:3]}\")\n    import os\n    if os.path.exists('/content/nooa-transformers/grammatical_transformers'):\n        print(f\"   ‚úì Diret√≥rio existe: /content/nooa-transformers/grammatical_transformers\")\n    if os.path.exists('/content/nooa-transformers/setup.py'):\n        print(f\"   ‚úì setup.py existe na raiz\")\n    raise\n\n# Verificar outras depend√™ncias\ntry:\n    import datasets\n    import transformers\n    import scipy\n    print(f\"‚úÖ datasets: {datasets.__version__}\")\n    print(f\"‚úÖ transformers: {transformers.__version__}\")\n    print(f\"‚úÖ scipy: {scipy.__version__}\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è  Depend√™ncia faltando: {e}\")\n\nprint(\"\\n‚úÖ Instala√ß√£o completa!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ C√©lula 4: Teste R√°pido\n",
    "\n",
    "Verifica que o modelo funciona antes de treinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammatical_transformers import GrammaticalBertModel, GrammaticalBertConfig\n",
    "import torch\n",
    "\n",
    "print(\"üîß Criando modelo de teste...\")\n",
    "config = GrammaticalBertConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=256,  # Pequeno para teste r√°pido\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    constituency_penalty=0.5\n",
    ")\n",
    "model = GrammaticalBertModel(config)\n",
    "\n",
    "# Teste forward pass\n",
    "print(\"üß™ Testando forward pass...\")\n",
    "input_ids = torch.randint(0, 30522, (2, 16))\n",
    "outputs = model(input_ids=input_ids)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo funciona!\")\n",
    "print(f\"   Output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"   Constituency trees: {len(outputs.constituency_trees)} exemplos\")\n",
    "\n",
    "# Limpar mem√≥ria\n",
    "del model, outputs\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"\\nüéâ Tudo pronto para treinar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ C√©lula 5: Treinar no SST-2 (Sentiment Analysis)\n",
    "\n",
    "**Tempo estimado**: ~20 minutos no T4\n",
    "\n",
    "**O que vai acontecer**:\n",
    "1. Baixar dataset SST-2 automaticamente (67K exemplos)\n",
    "2. Treinar por 3 epochs\n",
    "3. Avaliar no validation set\n",
    "4. Mostrar accuracy e F1 score\n",
    "\n",
    "**AGUARDE** at√© aparecer os resultados finais!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Voltar para o diret√≥rio raiz do projeto\n",
    "os.chdir('/content/nooa-transformers')\n",
    "print(f\"üìÇ Executando de: {os.getcwd()}\\n\")\n",
    "\n",
    "# Treinar no SST-2 usando m√≥dulo Python\n",
    "print(\"üöÄ Iniciando treinamento no SST-2...\\n\")\n",
    "!python -m grammatical_transformers.benchmarks.glue_test \\\n",
    "  --task sst2 \\\n",
    "  --epochs 3 \\\n",
    "  --batch_size 32 \\\n",
    "  --lr 2e-5 \\\n",
    "  --constituency_penalty 0.5\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TREINAMENTO COMPLETO!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Confira os resultados acima:\")\n",
    "print(\"   - Accuracy: quanto acertou\")\n",
    "print(\"   - F1 Score: m√©dia harm√¥nica de precision e recall\")\n",
    "print(\"   - Training time: tempo total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä C√©lula 6: Comparar com Vanilla BERT (Opcional)\n",
    "\n",
    "Compara GrammaticalBERT vs BERT padr√£o em 1000 exemplos.\n",
    "\n",
    "**Tempo**: ~10 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Garantir que estamos no diret√≥rio raiz\n",
    "os.chdir('/content/nooa-transformers')\n",
    "print(f\"üìÇ Executando de: {os.getcwd()}\\n\")\n",
    "\n",
    "# Comparar com vanilla BERT\n",
    "print(\"üìä Comparando GrammaticalBERT vs Vanilla BERT...\\n\")\n",
    "!python -m grammatical_transformers.benchmarks.compare_vanilla \\\n",
    "  --task sst2 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_samples 1000\n",
    "\n",
    "print(\"\\nüìä Veja a compara√ß√£o acima:\")\n",
    "print(\"   - Performance: accuracy, F1\")\n",
    "print(\"   - Efficiency: tempo, mem√≥ria\")\n",
    "print(\"   - Interpretability: attention entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç C√©lula 7: Teste de Hallucination Detection (Opcional)\n",
    "\n",
    "Testa a capacidade do modelo de detectar inconsist√™ncias.\n",
    "\n",
    "**Tempo**: ~5 minutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Garantir que estamos no diret√≥rio raiz\n",
    "os.chdir('/content/nooa-transformers')\n",
    "print(f\"üìÇ Executando de: {os.getcwd()}\\n\")\n",
    "\n",
    "# Teste de hallucination\n",
    "print(\"üîç Testando detec√ß√£o de hallucinations...\\n\")\n",
    "!python -m grammatical_transformers.benchmarks.hallucination_test\n",
    "\n",
    "print(\"\\nüéØ M√©tricas de hallucination:\")\n",
    "print(\"   - Entity preservation: mant√©m entidades corretas\")\n",
    "print(\"   - Predicate preservation: mant√©m predicados\")\n",
    "print(\"   - Negation consistency: detecta nega√ß√µes\")\n",
    "print(\"   - Overall symmetry: simetria geral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ C√©lula 8: Uso Interativo\n",
    "\n",
    "Use o modelo treinado para classificar seus pr√≥prios textos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammatical_transformers import (\n",
    "    GrammaticalBertForSequenceClassification,\n",
    "    GrammaticalBertConfig\n",
    ")\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"üîß Carregando modelo...\\n\")\n",
    "\n",
    "# Verificar se existe modelo treinado salvo\n",
    "model_dir = \"/content/glue_results/sst2\"  # Diret√≥rio onde o treinamento salva\n",
    "saved_model_dir = \"/content/grammatical_bert_sst2\"\n",
    "\n",
    "# Tentar carregar modelo treinado\n",
    "model_loaded = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Op√ß√£o 1: Carregar do diret√≥rio de treinamento\n",
    "if os.path.exists(model_dir) and os.path.exists(f\"{model_dir}/pytorch_model.bin\"):\n",
    "    print(f\"‚úÖ Encontrado modelo treinado em {model_dir}\")\n",
    "    try:\n",
    "        config = GrammaticalBertConfig.from_pretrained(model_dir)\n",
    "        model = GrammaticalBertForSequenceClassification.from_pretrained(model_dir, config=config)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        model_loaded = True\n",
    "        print(\"‚úÖ Modelo treinado carregado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erro ao carregar: {e}\")\n",
    "\n",
    "# Op√ß√£o 2: Carregar do diret√≥rio salvo manualmente\n",
    "elif os.path.exists(saved_model_dir):\n",
    "    print(f\"‚úÖ Encontrado modelo salvo em {saved_model_dir}\")\n",
    "    try:\n",
    "        config = GrammaticalBertConfig.from_pretrained(saved_model_dir)\n",
    "        model = GrammaticalBertForSequenceClassification.from_pretrained(saved_model_dir, config=config)\n",
    "        tokenizer = BertTokenizer.from_pretrained(saved_model_dir)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        model_loaded = True\n",
    "        print(\"‚úÖ Modelo salvo carregado com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erro ao carregar: {e}\")\n",
    "\n",
    "# Op√ß√£o 3: Criar modelo com pesos pr√©-treinados do BERT (n√£o fine-tunado)\n",
    "if not model_loaded:\n",
    "    print(\"‚ö†Ô∏è  Nenhum modelo treinado encontrado.\")\n",
    "    print(\"üìù Criando modelo com pesos pr√©-treinados do BERT (n√£o fine-tunado no SST-2)...\")\n",
    "    print(\"   Para usar o modelo treinado, execute a C√©lula 5 primeiro!\\n\")\n",
    "    \n",
    "    # Usar AutoModel para carregar pesos do BERT\n",
    "    from transformers import AutoModel\n",
    "    \n",
    "    config = GrammaticalBertConfig.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels=2,\n",
    "        constituency_penalty=0.5\n",
    "    )\n",
    "    \n",
    "    model = GrammaticalBertForSequenceClassification(config)\n",
    "    \n",
    "    # Carregar pesos do BERT base (encoder)\n",
    "    bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    model.bert.load_state_dict(bert_model.state_dict(), strict=False)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"‚ö†Ô∏è  Aten√ß√£o: Este modelo N√ÉO foi fine-tunado! As previs√µes ser√£o aleat√≥rias.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo pronto! (device: {device})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    \"\"\"\n",
    "    Classifica sentimento de um texto\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    label = \"üòä Positive\" if pred.item() == 1 else \"üòû Negative\"\n",
    "    confidence = probs[0, pred.item()].item()\n",
    "    \n",
    "    print(f\"\\nüìù Text: {text}\")\n",
    "    print(f\"üéØ Sentiment: {label}\")\n",
    "    print(f\"üìä Confidence: {confidence:.1%}\")\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Testar com exemplos\n",
    "print(\"üß™ Testando exemplos:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "examples = [\n",
    "    \"This movie is absolutely amazing!\",\n",
    "    \"I hated every minute of it.\",\n",
    "    \"The plot was confusing but the acting was great.\",\n",
    "    \"Best film I've seen this year!\",\n",
    "    \"Waste of time and money.\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    classify_sentiment(text)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifique seu pr√≥prio texto!\n",
    "# Mude o texto abaixo e execute a c√©lula:\n",
    "\n",
    "my_text = \"This project is incredible and revolutionary!\"\n",
    "\n",
    "classify_sentiment(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ C√©lula 9: Salvar Modelo\n",
    "\n",
    "Salva o modelo treinado para usar depois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Diret√≥rio onde o treinamento salva os checkpoints\n",
    "training_output_dir = \"/content/glue_results/sst2\"\n",
    "# Diret√≥rio final para salvar\n",
    "final_output_dir = \"/content/grammatical_bert_sst2\"\n",
    "\n",
    "print(\"üíæ Salvando modelo...\\n\")\n",
    "\n",
    "# Verificar se existe modelo treinado\n",
    "if os.path.exists(training_output_dir):\n",
    "    # Encontrar o melhor checkpoint (geralmente na pasta raiz ap√≥s treinamento)\n",
    "    checkpoint_dirs = [d for d in os.listdir(training_output_dir) if d.startswith('checkpoint-')]\n",
    "    \n",
    "    if os.path.exists(f\"{training_output_dir}/pytorch_model.bin\"):\n",
    "        # Modelo j√° est√° na raiz (best model)\n",
    "        source_dir = training_output_dir\n",
    "        print(f\"‚úÖ Encontrado melhor modelo em {source_dir}\")\n",
    "    elif checkpoint_dirs:\n",
    "        # Pegar √∫ltimo checkpoint\n",
    "        checkpoint_dirs.sort()\n",
    "        source_dir = os.path.join(training_output_dir, checkpoint_dirs[-1])\n",
    "        print(f\"‚úÖ Encontrado checkpoint em {source_dir}\")\n",
    "    else:\n",
    "        print(\"‚ùå Nenhum modelo treinado encontrado!\")\n",
    "        print(\"   Execute a C√©lula 5 (treinamento) primeiro.\")\n",
    "        source_dir = None\n",
    "    \n",
    "    if source_dir:\n",
    "        # Copiar para diret√≥rio final\n",
    "        if os.path.exists(final_output_dir):\n",
    "            shutil.rmtree(final_output_dir)\n",
    "        shutil.copytree(source_dir, final_output_dir)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Modelo salvo em {final_output_dir}\")\n",
    "        print(\"\\nüì¶ Arquivos salvos:\")\n",
    "        !ls -lh {final_output_dir}\n",
    "        \n",
    "        print(\"\\nüí° Para baixar:\")\n",
    "        print(\"   1. Files (√† esquerda) ‚Üí content ‚Üí grammatical_bert_sst2\")\n",
    "        print(\"   2. Clique com direito ‚Üí Download\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum resultado de treinamento encontrado!\")\n",
    "    print(\"   Execute a C√©lula 5 (treinamento no SST-2) primeiro.\")\n",
    "    print(f\"   Procurando em: {training_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è C√©lula 10: Upload para Google Drive (Opcional)\n",
    "\n",
    "Salva no seu Google Drive para n√£o perder quando sess√£o expirar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copiar modelo para Drive\n",
    "!cp -r /content/grammatical_bert_sst2 /content/drive/MyDrive/\n",
    "\n",
    "print(\"\\n‚úÖ Modelo copiado para Google Drive!\")\n",
    "print(\"üìÅ Localiza√ß√£o: MyDrive/grammatical_bert_sst2\")\n",
    "print(\"\\nüí° Agora voc√™ pode:\")\n",
    "print(\"   - Acessar de qualquer lugar\")\n",
    "print(\"   - Carregar em outro notebook\")\n",
    "print(\"   - Baixar no seu computador\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Parab√©ns!\n",
    "\n",
    "Voc√™ treinou com sucesso o GrammaticalBERT!\n",
    "\n",
    "### üìä O que voc√™ fez:\n",
    "\n",
    "‚úÖ Treinou modelo em 67K exemplos  \n",
    "‚úÖ Obteve accuracy ~91-93% no SST-2  \n",
    "‚úÖ Comparou com vanilla BERT (opcional)  \n",
    "‚úÖ Testou hallucination detection (opcional)  \n",
    "‚úÖ Usou o modelo interativamente  \n",
    "‚úÖ Salvou para usar depois  \n",
    "\n",
    "### üöÄ Pr√≥ximos Passos:\n",
    "\n",
    "1. **Testar outras tarefas GLUE**:\n",
    "   ```python\n",
    "   # Execute em uma nova c√©lula, certificando-se de estar em /content/nooa-transformers:\n",
    "   !python -m grammatical_transformers.benchmarks.glue_test --task cola --epochs 3\n",
    "   !python -m grammatical_transformers.benchmarks.glue_test --task mnli --epochs 3\n",
    "   ```\n",
    "\n",
    "2. **Ajustar hiperpar√¢metros**:\n",
    "   - `constituency_penalty`: 0.3, 0.5, 0.7 (teste diferentes valores)\n",
    "   - `learning_rate`: 1e-5, 2e-5, 5e-5\n",
    "   - `epochs`: 3, 5, 10\n",
    "\n",
    "3. **Publicar resultados**:\n",
    "   - Atualizar RESULTS.md no GitHub\n",
    "   - Compartilhar descobertas\n",
    "   - Contribuir com melhorias\n",
    "\n",
    "4. **Upload para Hugging Face Hub**:\n",
    "   - Compartilhar modelo treinado\n",
    "   - Outros podem usar seu modelo\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Troubleshooting:\n",
    "\n",
    "**Problema: \"ModuleNotFoundError: No module named 'grammatical_transformers'\"**\n",
    "- Solu√ß√£o: Execute a C√©lula 3 novamente para instalar o pacote\n",
    "\n",
    "**Problema: \"FileNotFoundError\" ao executar benchmarks**\n",
    "- Solu√ß√£o: Certifique-se de estar no diret√≥rio `/content/nooa-transformers`\n",
    "- Execute: `import os; os.chdir('/content/nooa-transformers'); print(os.getcwd())`\n",
    "\n",
    "**Problema: \"CUDA out of memory\"**\n",
    "- Solu√ß√£o: Reduza `batch_size` para 16 ou 8 nas c√©lulas de treinamento\n",
    "\n",
    "**Problema: Modelo dando previs√µes aleat√≥rias na C√©lula 8**\n",
    "- Solu√ß√£o: Execute a C√©lula 5 (treinamento) primeiro para ter um modelo treinado\n",
    "\n",
    "**Problema: Sess√£o desconectou durante treinamento**\n",
    "- Solu√ß√£o: \n",
    "  1. Reconecte ao runtime\n",
    "  2. Execute C√©lula 1, 2, 3 novamente\n",
    "  3. Continue do ponto onde parou (modelos salvos em `/content/glue_results/`)\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Recursos:\n",
    "\n",
    "- **GitHub**: https://github.com/nooa-ai/nooa-transformers\n",
    "- **Documenta√ß√£o**: README.md, ARCHITECTURE.md\n",
    "- **Issues**: Reporte problemas ou pe√ßa ajuda\n",
    "\n",
    "---\n",
    "\n",
    "**üß† \"Grammar is compression.\" - This project**\n",
    "\n",
    "**LFG! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}