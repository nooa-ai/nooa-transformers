{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ GrammaticalBERT Training - Google Colab\n",
    "\n",
    "Este notebook treina o GrammaticalBERT no Google Colab usando GPU gr√°tis!\n",
    "\n",
    "## Setup R√°pido:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
    "2. Execute as c√©lulas em ordem (Shift+Enter)\n",
    "3. Aguarde ~20 minutos para treinar no SST-2\n",
    "\n",
    "## O que vamos fazer:\n",
    "- Fine-tuning do GrammaticalBERT no dataset SST-2 (sentiment analysis)\n",
    "- Comparar com vanilla BERT\n",
    "- Medir accuracy, F1, e redu√ß√£o de hallucinations\n",
    "\n",
    "**Dataset**: Baixa automaticamente (sem prepara√ß√£o manual!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Verificar GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se GPU est√° dispon√≠vel\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ GPU dispon√≠vel: {gpu_name}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU n√£o dispon√≠vel! V√° em Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clonar Reposit√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar o reposit√≥rio\n",
    "!git clone https://github.com/nooa-ai/nooa-transformers.git\n",
    "%cd nooa-transformers/grammatical_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Instalar Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar pacote\n",
    "!pip install -e . -q\n",
    "!pip install datasets accelerate -q\n",
    "\n",
    "print(\"‚úÖ Instala√ß√£o completa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Teste R√°pido (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste r√°pido para verificar que tudo funciona\n",
    "from grammatical_transformers import GrammaticalBertModel, GrammaticalBertConfig\n",
    "import torch\n",
    "\n",
    "print(\"üîß Criando modelo de teste...\")\n",
    "config = GrammaticalBertConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=4,  # Pequeno para teste r√°pido\n",
    "    num_attention_heads=12,\n",
    "    constituency_penalty=0.5\n",
    ")\n",
    "model = GrammaticalBertModel(config)\n",
    "\n",
    "# Teste forward pass\n",
    "input_ids = torch.randint(0, 30522, (2, 32))\n",
    "outputs = model(input_ids=input_ids)\n",
    "\n",
    "print(f\"‚úÖ Modelo funciona! Output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"   Constituency trees: {len(outputs.constituency_trees)} exemplos\")\n",
    "print(f\"   Symmetry scores: {outputs.symmetry_scores.shape if outputs.symmetry_scores is not None else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Op√ß√£o A: Fine-tuning Simples no SST-2 (Recomendado)\n",
    "\n",
    "**SST-2**: Sentiment Analysis (positive/negative)\n",
    "- 67K exemplos de treinamento\n",
    "- Tempo estimado: ~20 minutos no T4\n",
    "- Dataset baixa automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar no SST-2\n",
    "!python benchmarks/glue_test.py \\\n",
    "  --task sst2 \\\n",
    "  --epochs 3 \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --constituency_penalty 0.5 \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\n‚úÖ Treinamento completo!\")\n",
    "print(\"üìä Confira os resultados acima (accuracy, F1 score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Op√ß√£o B: Compara√ß√£o GrammaticalBERT vs Vanilla BERT\n",
    "\n",
    "Executa benchmark completo comparando os dois modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar com vanilla BERT\n",
    "!python benchmarks/compare_vanilla.py \\\n",
    "  --task sst2 \\\n",
    "  --batch_size 32 \\\n",
    "  --num_samples 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Teste de Hallucination Detection\n",
    "\n",
    "Testa a capacidade do modelo de detectar inconsist√™ncias (hallucinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de hallucination\n",
    "!python benchmarks/hallucination_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Uso Interativo (Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo treinado para uso\n",
    "from grammatical_transformers import (\n",
    "    GrammaticalBertForSequenceClassification,\n",
    "    GrammaticalBertConfig\n",
    ")\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Config\n",
    "config = GrammaticalBertConfig(\n",
    "    vocab_size=30522,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    num_labels=2,  # SST-2: positive/negative\n",
    "    constituency_penalty=0.5\n",
    ")\n",
    "\n",
    "# Modelo e tokenizer\n",
    "model = GrammaticalBertForSequenceClassification(config)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Mover para GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Modelo pronto para infer√™ncia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classificar sentimentos\n",
    "def classify_sentiment(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    label = \"Positive\" if pred.item() == 1 else \"Negative\"\n",
    "    confidence = probs[0, pred.item()].item()\n",
    "    \n",
    "    print(f\"\\nüìù Text: {text}\")\n",
    "    print(f\"üéØ Sentiment: {label}\")\n",
    "    print(f\"üìä Confidence: {confidence:.2%}\")\n",
    "    \n",
    "    # Mostrar constituency tree se dispon√≠vel\n",
    "    if hasattr(outputs, 'constituency_trees') and outputs.constituency_trees:\n",
    "        tree = outputs.constituency_trees[0]\n",
    "        print(f\"üå≥ Constituency Tree: {tree}\")\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Exemplos\n",
    "examples = [\n",
    "    \"This movie is absolutely amazing!\",\n",
    "    \"I hated every minute of it.\",\n",
    "    \"The plot was confusing but the acting was great.\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    classify_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualizar Constituency Trees (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar estrutura gramatical\n",
    "def visualize_constituency(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    if hasattr(outputs, 'constituency_trees') and outputs.constituency_trees:\n",
    "        tree = outputs.constituency_trees[0]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        print(f\"\\nüìù Sentence: {text}\")\n",
    "        print(f\"üî§ Tokens: {' '.join(tokens)}\")\n",
    "        print(f\"üå≥ Constituency Tree:\\n{tree}\")\n",
    "    else:\n",
    "        print(\"No constituency tree available\")\n",
    "\n",
    "# Testar\n",
    "visualize_constituency(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Salvar Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo\n",
    "output_dir = \"./grammatical_bert_sst2\"\n",
    "model.save_pretrained(output_dir)\n",
    "config.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Modelo salvo em {output_dir}\")\n",
    "print(\"\\nüì¶ Para baixar, v√° em Files (√† esquerda) ‚Üí Clique com direito ‚Üí Download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Upload para Google Drive (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copiar modelo para Drive\n",
    "!cp -r ./grammatical_bert_sst2 /content/drive/MyDrive/\n",
    "\n",
    "print(\"‚úÖ Modelo copiado para Google Drive!\")\n",
    "print(\"üìÅ Localiza√ß√£o: MyDrive/grammatical_bert_sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Pr√≥ximos Passos\n",
    "\n",
    "Agora voc√™ pode:\n",
    "\n",
    "1. **Testar outras tarefas GLUE**:\n",
    "   ```python\n",
    "   !python benchmarks/glue_test.py --task cola  # Gramaticalidade\n",
    "   !python benchmarks/glue_test.py --task mnli  # Entailment\n",
    "   ```\n",
    "\n",
    "2. **Comparar resultados**: Execute `compare_vanilla.py` para ver diferen√ßas\n",
    "\n",
    "3. **Publicar resultados**: Atualize `RESULTS.md` no GitHub\n",
    "\n",
    "4. **Upload para Hugging Face Hub**: Compartilhe modelo treinado\n",
    "\n",
    "5. **Escrever paper**: Documente descobertas\n",
    "\n",
    "---\n",
    "\n",
    "**Problemas?** Veja: https://github.com/nooa-ai/nooa-transformers/issues\n",
    "\n",
    "**LFG!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
