# Google Colab - Copiar e Colar (5 minutos)

## ğŸ¯ Objetivo
Treinar GrammaticalBERT no SST-2 (sentiment analysis) em 20 minutos usando GPU grÃ¡tis.

**Resposta rÃ¡pida**: âŒ NÃƒO precisa preparar dataset! Tudo automÃ¡tico.

---

## ğŸ“‹ Passo a Passo

### 1. Abrir Google Colab
ğŸ‘‰ https://colab.research.google.com

### 2. Ativar GPU GrÃ¡tis
1. No menu: **Runtime â†’ Change runtime type**
2. **Hardware accelerator**: GPU
3. **Save**

### 3. Copiar e Colar (uma cÃ©lula de cada vez)

#### CÃ©lula 1: Verificar GPU
```python
import torch
print(f"âœ… GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'âŒ NÃ£o disponÃ­vel'}")
```

#### CÃ©lula 2: Clonar repo
```python
!git clone https://github.com/nooa-ai/nooa-transformers.git
%cd nooa-transformers/grammatical_transformers
```

#### CÃ©lula 3: Instalar
```python
!pip install -e . -q
!pip install datasets accelerate -q
print("âœ… Pronto!")
```

#### CÃ©lula 4: Treinar (aguarde ~20 min)
```python
!python benchmarks/glue_test.py \
  --task sst2 \
  --epochs 3 \
  --batch_size 32 \
  --device cuda
```

#### CÃ©lula 5: Ver resultados
```python
# Resultados aparecem automaticamente na cÃ©lula anterior
# Procure por:
# - Accuracy: ~91-93%
# - F1 Score: ~91-93%
# - Training time: ~20 minutos
```

---

## ğŸ‰ Pronto!

VocÃª acabou de treinar o GrammaticalBERT!

### ğŸ“Š O que aconteceu?

1. âœ… **Dataset baixado automaticamente**: 67K exemplos do SST-2
2. âœ… **Modelo treinado**: 3 epochs (~20 min)
3. âœ… **Resultados**: Accuracy e F1 score

### ğŸ”¥ PrÃ³ximos Passos

#### OpÃ§Ã£o A: Comparar com vanilla BERT
```python
!python benchmarks/compare_vanilla.py --task sst2 --num_samples 1000
```

#### OpÃ§Ã£o B: Testar hallucination detection
```python
!python benchmarks/hallucination_test.py
```

#### OpÃ§Ã£o C: Testar outras tarefas GLUE
```python
# Gramaticalidade (interessante para GrammaticalBERT!)
!python benchmarks/glue_test.py --task cola

# Natural Language Inference (mais desafiador)
!python benchmarks/glue_test.py --task mnli
```

---

## ğŸ†˜ Problemas Comuns

### "GPU not available"
**SoluÃ§Ã£o**: Runtime â†’ Change runtime type â†’ GPU â†’ Save

### "Out of memory"
**SoluÃ§Ã£o**: Reduzir batch_size:
```python
!python benchmarks/glue_test.py --task sst2 --batch_size 16  # ou 8
```

### "Session disconnected"
**Causa**: Colab free desconecta apÃ³s 12h ou inatividade
**SoluÃ§Ã£o**: Reabrir e rodar novamente (progresso salvo se vocÃª salvou o modelo)

---

## ğŸ’¾ Salvar Modelo Treinado

```python
# Salvar localmente no Colab
from grammatical_transformers import GrammaticalBertForSequenceClassification
model.save_pretrained("./my_model")

# Copiar para Google Drive
from google.colab import drive
drive.mount('/content/drive')
!cp -r ./my_model /content/drive/MyDrive/grammatical_bert_sst2
```

Depois baixe do Drive ou use em outro notebook!

---

## ğŸ“– Mais InformaÃ§Ãµes

- **Notebook completo**: `colab_training.ipynb` (mais exemplos)
- **Guia detalhado**: `COLAB_QUICKSTART.md`
- **Hardware**: `TRAINING_GUIDE.md`
- **DocumentaÃ§Ã£o**: `README.md`

---

## ğŸš€ LFG!

**Tempo total**: ~25 minutos (5 min setup + 20 min training)
**Custo**: $0 (grÃ¡tis!)
**PreparaÃ§Ã£o de dataset**: Zero, tudo automÃ¡tico!
