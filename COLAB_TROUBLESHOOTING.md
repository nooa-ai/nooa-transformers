# Google Colab - Troubleshooting

Problemas comuns e solu√ß√µes ao treinar GrammaticalBERT no Colab.

---

## ‚ùå Erro: "No module named 'grammatical_transformers'"

### Causa
Voc√™ pulou a c√©lula de instala√ß√£o ou ela falhou.

### Solu√ß√£o 1: Execute a c√©lula de instala√ß√£o
```python
# C√©lula 3: Instalar Depend√™ncias
!pip install -e .
```

Aguarde at√© ver "‚úÖ Instala√ß√£o completa!"

### Solu√ß√£o 2: Verificar diret√≥rio
```python
import os
print(os.getcwd())
# Deve ser: /content/nooa-transformers/grammatical_transformers
```

Se n√£o estiver no diret√≥rio correto:
```python
%cd /content/nooa-transformers/grammatical_transformers
```

### Solu√ß√£o 3: Instalar manualmente
```python
import sys
sys.path.insert(0, '/content/nooa-transformers/grammatical_transformers')

# Tentar importar novamente
from grammatical_transformers import GrammaticalBertModel
```

### Solu√ß√£o 4: Reinstalar tudo
```python
# Limpar instala√ß√£o anterior
!pip uninstall grammatical-transformers -y

# Reinstalar
%cd /content/nooa-transformers/grammatical_transformers
!pip install -e .
```

---

## ‚ùå Erro: "GPU not available"

### Causa
GPU n√£o est√° ativada ou sess√£o perdeu GPU.

### Solu√ß√£o 1: Ativar GPU
1. Menu: **Runtime ‚Üí Change runtime type**
2. **Hardware accelerator**: GPU
3. **Save**
4. Reiniciar runtime: **Runtime ‚Üí Restart runtime**

### Solu√ß√£o 2: Verificar disponibilidade
```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPU name: {torch.cuda.get_device_name(0)}")
```

### Solu√ß√£o 3: Colab Free esgotou GPU
**Problema**: Colab free tem limite de uso de GPU.

**Op√ß√µes**:
- Aguardar algumas horas
- Usar CPU (muito lento): `--device cpu`
- Upgrade para Colab Pro ($10/m√™s)

---

## ‚ùå Erro: "CUDA out of memory"

### Causa
Modelo muito grande para VRAM dispon√≠vel.

### Solu√ß√£o 1: Reduzir batch_size
```python
# Em vez de batch_size=32
!python benchmarks/glue_test.py --task sst2 --batch_size 16 --device cuda

# Se ainda der erro, tente 8
!python benchmarks/glue_test.py --task sst2 --batch_size 8 --device cuda
```

### Solu√ß√£o 2: Reduzir tamanho do modelo
```python
config = GrammaticalBertConfig(
    vocab_size=30522,
    hidden_size=512,      # Reduzido de 768
    num_hidden_layers=6,  # Reduzido de 12
    num_attention_heads=8,  # Reduzido de 12
)
```

### Solu√ß√£o 3: Limpar cache CUDA
```python
import torch
torch.cuda.empty_cache()
```

### Solu√ß√£o 4: Reiniciar runtime
**Runtime ‚Üí Restart runtime** e execute c√©lulas novamente.

---

## ‚ùå Erro: "Session disconnected"

### Causa
Colab free desconecta ap√≥s:
- 12 horas de uso
- Inatividade prolongada
- Uso excessivo de recursos

### Solu√ß√£o 1: Prevenir desconex√£o
Execute este c√≥digo em uma c√©lula:
```python
# Anti-desconex√£o (execute em c√©lula separada)
import time
from IPython.display import clear_output

for i in range(1000):
    time.sleep(60)  # Espera 1 minuto
    clear_output(wait=True)
    print(f"Mantendo sess√£o ativa: {i+1} minuto(s)")
```

### Solu√ß√£o 2: Salvar checkpoints periodicamente
Modifique o treinamento para salvar a cada epoch:
```python
# No seu script de treinamento
for epoch in range(num_epochs):
    train_epoch()
    model.save_pretrained(f"checkpoint_epoch_{epoch}")
```

### Solu√ß√£o 3: Usar Google Drive
Salve modelo no Drive automaticamente:
```python
from google.colab import drive
drive.mount('/content/drive')

# Ap√≥s treinar
!cp -r ./model /content/drive/MyDrive/checkpoints/
```

---

## ‚ùå Erro: "Repository not found" ao clonar

### Causa
Reposit√≥rio ainda n√£o est√° p√∫blico ou URL errada.

### Solu√ß√£o 1: Verificar URL
```python
# URL correta
!git clone https://github.com/nooa-ai/nooa-transformers.git
```

### Solu√ß√£o 2: Verificar se reposit√≥rio existe
Abra no navegador: https://github.com/nooa-ai/nooa-transformers

### Solu√ß√£o 3: Clonar de fork
Se voc√™ tem um fork:
```python
!git clone https://github.com/SEU-USUARIO/nooa-transformers.git
```

---

## ‚ùå Erro: "Dataset download failed"

### Causa
Problema de conex√£o ou dataset n√£o encontrado.

### Solu√ß√£o 1: Tentar novamente
```python
from datasets import load_dataset
dataset = load_dataset("glue", "sst2")
```

### Solu√ß√£o 2: Cache pode estar corrompido
```python
# Limpar cache
!rm -rf ~/.cache/huggingface/datasets/

# Baixar novamente
from datasets import load_dataset
dataset = load_dataset("glue", "sst2")
```

### Solu√ß√£o 3: Verificar conex√£o
```python
!ping google.com -c 3
!curl https://huggingface.co
```

---

## ‚ùå Erro: "ModuleNotFoundError: torch"

### Causa
PyTorch n√£o instalado ou vers√£o incompat√≠vel.

### Solu√ß√£o
```python
# Desinstalar vers√£o antiga
!pip uninstall torch torchvision torchaudio -y

# Instalar vers√£o compat√≠vel com CUDA
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verificar instala√ß√£o
import torch
print(torch.__version__)
print(torch.cuda.is_available())
```

---

## ‚ö†Ô∏è Aviso: "Running on CPU"

### Causa
GPU n√£o ativada ou n√£o dispon√≠vel.

### Solu√ß√£o
Veja se√ß√£o "GPU not available" acima.

### Alternativa: Aceitar CPU (lento)
```python
# Treinar em CPU (vai levar 5-10x mais tempo)
!python benchmarks/glue_test.py --task sst2 --device cpu --batch_size 8
```

---

## ‚ö†Ô∏è Aviso: Training muito lento

### Diagn√≥stico
```python
import torch
print(f"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}")

# Se T4:  ~20 min para SST-2 (normal)
# Se P100: ~15 min (r√°pido)
# Se CPU:  ~2-3 horas (muito lento)
```

### Solu√ß√£o 1: Verificar que est√° usando GPU
```python
# No c√≥digo de treinamento
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
```

### Solu√ß√£o 2: Usar batch_size maior
```python
# Se VRAM permitir
!python benchmarks/glue_test.py --task sst2 --batch_size 64 --device cuda
```

### Solu√ß√£o 3: Reduzir logging
```python
# Menos prints = mais r√°pido
!python benchmarks/glue_test.py --task sst2 --quiet
```

---

## üîß Debugging Geral

### Verificar ambiente completo
```python
import sys
import torch
from transformers import __version__ as transformers_version

print("=== Ambiente ===" )
print(f"Python: {sys.version}")
print(f"PyTorch: {torch.__version__}")
print(f"Transformers: {transformers_version}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print(f"Working dir: {sys.path[0]}")

# Verificar instala√ß√£o do pacote
try:
    import grammatical_transformers
    print(f"\n‚úÖ grammatical_transformers: v{grammatical_transformers.__version__}")
except ImportError as e:
    print(f"\n‚ùå grammatical_transformers: {e}")
```

### Teste m√≠nimo
```python
# Teste mais simples poss√≠vel
from grammatical_transformers import GrammaticalBertConfig
import torch

config = GrammaticalBertConfig(vocab_size=100, hidden_size=64, num_hidden_layers=1)
print(f"‚úÖ Config criado: {config}")
```

### Habilitar modo debug
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## üìû Pedir Ajuda

Se nenhuma solu√ß√£o funcionou:

### 1. Criar Issue no GitHub
https://github.com/nooa-ai/nooa-transformers/issues

**Inclua**:
- Erro completo (screenshot ou texto)
- C√≥digo que voc√™ executou
- Output de `verificar ambiente completo` (acima)
- Qual c√©lula falhou

### 2. Copiar este template

```
## Problema
[Descreva o problema]

## Erro
```
[Cole o erro completo aqui]
```

## Ambiente
- GPU: [T4/P100/None]
- Colab: [Free/Pro]
- Python: [vers√£o]
- PyTorch: [vers√£o]

## C√≥digo
```python
[Cole o c√≥digo que causou o erro]
```

## O que j√° tentei
- [ ] Reinstalei pacote
- [ ] Reiniciei runtime
- [ ] Verifiquei GPU est√° ativa
- [ ] ...
```

---

## üéì Dicas Gerais

### ‚úÖ Sempre execute as c√©lulas em ordem
N√£o pule c√©lulas, especialmente instala√ß√£o.

### ‚úÖ Aguarde instala√ß√£o completa
A c√©lula 3 (instala√ß√£o) pode levar 2-3 minutos.

### ‚úÖ Reinicie runtime ap√≥s instalar
√Äs vezes √© necess√°rio: **Runtime ‚Üí Restart runtime**

### ‚úÖ Salve checkpoints frequentemente
Use Google Drive para n√£o perder progresso.

### ‚úÖ Monitore recursos
```python
# Ver uso de GPU
!nvidia-smi

# Ver uso de RAM
!free -h

# Ver uso de disco
!df -h
```

---

## üìö Recursos Adicionais

- **Colab FAQ**: https://research.google.com/colaboratory/faq.html
- **PyTorch Troubleshooting**: https://pytorch.org/get-started/locally/
- **Hugging Face Datasets**: https://huggingface.co/docs/datasets/

---

**√öltima atualiza√ß√£o**: 2025-10-13
**Problemas n√£o listados?** Abra issue: https://github.com/nooa-ai/nooa-transformers/issues
